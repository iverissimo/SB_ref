{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tryout script to do the whole pipeline on the prf data of one sub\n",
    "# after try to generalize for all tasks \n",
    "# put in Nipype format (do mapNodes because then I can iterate throught files, nicer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from nilearn import image, plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from spynoza.filtering.nodes import Savgol_filter#, Savgol_filter_confounds\n",
    "from spynoza.conversion.nodes import Percent_signal_change\n",
    "from nipype import Node, Function\n",
    "import nipype.pipeline as pe\n",
    "import nipype.interfaces.io as nio\n",
    "from bids.grabbids import BIDSLayout\n",
    "\n",
    "#from nipype.interfaces import fsl\n",
    "#from nipype.interfaces import freesurfer\n",
    "#from nipype.interfaces.utility import Function, IdentityInterface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save relevant params used in json file, \n",
    "# afterwards just need to call the field of the list/dict\n",
    "import json\n",
    "analysis_params = {}\n",
    "json_info = open('SBref_analysis_params.json','r').read()\n",
    "analysis_params.update(json.loads(json_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions\n",
    "def SGfilt_confound(confounds, tr, polyorder=3, deriv=0, window_length=120):\n",
    "    import pandas as pd\n",
    "    from scipy.signal import savgol_filter\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    confounds_table = pd.read_csv(confounds, sep='\\t', na_values='n/a') #added this line to original spynoza func because it was giving error when str transformed to float\n",
    "    \n",
    "    window = np.int(window_length / tr)\n",
    "\n",
    "    # Window must be odd\n",
    "    if window % 2 == 0:\n",
    "        window += 1\n",
    "\n",
    "    confounds_filt = savgol_filter(confounds_table, window_length=window, polyorder=polyorder,\n",
    "                              deriv=deriv, axis=0, mode='nearest')\n",
    "\n",
    "    new_name = os.path.basename(confounds).split('.')[:-1][0] + '_sg.tsv'\n",
    "    out_file = os.path.abspath(new_name)\n",
    "\n",
    "    confounds_table = np.asarray(confounds_table).astype(np.float64) # needed to convert to float to support next operation\n",
    "    pd.DataFrame(confounds_table - confounds_filt).to_csv(out_file, sep='\\t', index=False)\n",
    "\n",
    "    return out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths and variables\n",
    "sub_list = ['2']\n",
    "ses_list = ['1']\n",
    "task='prf'\n",
    "\n",
    "source_pth = '/home/neuro/projects/data/derivatives/fmriprep/'\n",
    "dest_pth = '/home/neuro/projects/data/derivatives/post_fmriprep/'\n",
    "sub_num = str(sub_list[0]).zfill(2)\n",
    "ses_num = str(ses_list[0]).zfill(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets preprocessed niftis from derivative folder\n",
    "layout = BIDSLayout(source_pth)\n",
    "funcdata = layout.get(subject=sub_num,task=task,extensions='space-T1w_preproc.nii.gz',return_type='file') # list of paths to functionals for this sub\n",
    "confdata = layout.get(subject=sub_num,task=task,extensions='.tsv',return_type='file')  # list of paths to confounds for this sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/neuro/projects/data/derivatives/fmriprep/sub-02/ses-01/func/sub-02_ses-01_task-prf_run-01_bold_space-T1w_preproc.nii.gz',\n",
       " '/home/neuro/projects/data/derivatives/fmriprep/sub-02/ses-01/func/sub-02_ses-01_task-prf_run-02_bold_space-T1w_preproc.nii.gz',\n",
       " '/home/neuro/projects/data/derivatives/fmriprep/sub-02/ses-01/func/sub-02_ses-01_task-prf_run-03_bold_space-T1w_preproc.nii.gz',\n",
       " '/home/neuro/projects/data/derivatives/fmriprep/sub-02/ses-01/func/sub-02_ses-01_task-prf_run-04_bold_space-T1w_preproc.nii.gz',\n",
       " '/home/neuro/projects/data/derivatives/fmriprep/sub-02/ses-01/func/sub-02_ses-01_task-prf_run-05_bold_space-T1w_preproc.nii.gz']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some HP filtering (Savitzkyâ€“Golay) on bold + confounds\n",
    "# define Nodes\n",
    "SG_filter = pe.MapNode(interface=Savgol_filter,\n",
    "                       name='savgol_filt',\n",
    "                       iterfield=['in_file'])\n",
    "SG_filter_confounds = pe.MapNode(Function(function=SGfilt_confound),\n",
    "                       name='sgfilt_confound',\n",
    "                       iterfield=['confounds']) #iterfied needs different name because of temp dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define settings, right ones? \n",
    "SG_filter.inputs.polyorder = SG_filter_confounds.inputs.polyorder = analysis_params['sgfilter_polyorder']\n",
    "SG_filter.inputs.deriv = SG_filter_confounds.inputs.deriv = analysis_params['sgfilter_deriv']\n",
    "SG_filter.inputs.window_length = SG_filter_confounds.inputs.window_length = analysis_params['sgfilter_window_length']\n",
    "\n",
    "if (sub_num=='01' or sub_num=='03') and ses_num=='01': # exception for some initial subjects' sessions\n",
    "    SG_filter.inputs.tr = SG_filter_confounds.inputs.tr = 1.5\n",
    "else:\n",
    "    SG_filter.inputs.tr = SG_filter_confounds.inputs.tr = analysis_params['TR'] \n",
    "\n",
    "SG_filter.inputs.in_file = funcdata\n",
    "SG_filter_confounds.inputs.confounds = confdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190117-12:20:43,751 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"savgol_filt\" in \"/tmp/tmp77pv0_le/savgol_filt\".\n",
      "190117-12:20:43,766 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_savgol_filt0\" in \"/tmp/tmp77pv0_le/savgol_filt/mapflow/_savgol_filt0\".\n",
      "190117-12:20:43,774 nipype.workflow INFO:\n",
      "\t [Node] Running \"_savgol_filt0\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190117-12:20:57,744 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_savgol_filt0\".\n",
      "190117-12:20:57,753 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_savgol_filt1\" in \"/tmp/tmp77pv0_le/savgol_filt/mapflow/_savgol_filt1\".\n",
      "190117-12:20:57,758 nipype.workflow INFO:\n",
      "\t [Node] Running \"_savgol_filt1\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190117-12:21:11,87 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_savgol_filt1\".\n",
      "190117-12:21:11,92 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_savgol_filt2\" in \"/tmp/tmp77pv0_le/savgol_filt/mapflow/_savgol_filt2\".\n",
      "190117-12:21:11,97 nipype.workflow INFO:\n",
      "\t [Node] Running \"_savgol_filt2\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190117-12:21:24,632 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_savgol_filt2\".\n",
      "190117-12:21:24,637 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_savgol_filt3\" in \"/tmp/tmp77pv0_le/savgol_filt/mapflow/_savgol_filt3\".\n",
      "190117-12:21:24,642 nipype.workflow INFO:\n",
      "\t [Node] Running \"_savgol_filt3\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190117-12:21:39,134 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_savgol_filt3\".\n",
      "190117-12:21:39,138 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_savgol_filt4\" in \"/tmp/tmp77pv0_le/savgol_filt/mapflow/_savgol_filt4\".\n",
      "190117-12:21:39,143 nipype.workflow INFO:\n",
      "\t [Node] Running \"_savgol_filt4\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190117-12:21:53,50 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_savgol_filt4\".\n",
      "190117-12:21:53,55 nipype.workflow INFO:\n",
      "\t [Node] Finished \"savgol_filt\".\n"
     ]
    }
   ],
   "source": [
    "# run and print outputs\n",
    "res_SG_filter = SG_filter.run()\n",
    "#res_SG_filter.outputs.out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190117-12:22:19,226 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"sgfilt_confound\" in \"/tmp/tmpc7fvcwcc/sgfilt_confound\".\n",
      "190117-12:22:19,241 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_sgfilt_confound0\" in \"/tmp/tmpc7fvcwcc/sgfilt_confound/mapflow/_sgfilt_confound0\".\n",
      "190117-12:22:19,249 nipype.workflow INFO:\n",
      "\t [Node] Running \"_sgfilt_confound0\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190117-12:22:19,304 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_sgfilt_confound0\".\n",
      "190117-12:22:19,309 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_sgfilt_confound1\" in \"/tmp/tmpc7fvcwcc/sgfilt_confound/mapflow/_sgfilt_confound1\".\n",
      "190117-12:22:19,317 nipype.workflow INFO:\n",
      "\t [Node] Running \"_sgfilt_confound1\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190117-12:22:19,348 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_sgfilt_confound1\".\n",
      "190117-12:22:19,353 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_sgfilt_confound2\" in \"/tmp/tmpc7fvcwcc/sgfilt_confound/mapflow/_sgfilt_confound2\".\n",
      "190117-12:22:19,362 nipype.workflow INFO:\n",
      "\t [Node] Running \"_sgfilt_confound2\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190117-12:22:19,393 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_sgfilt_confound2\".\n",
      "190117-12:22:19,398 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_sgfilt_confound3\" in \"/tmp/tmpc7fvcwcc/sgfilt_confound/mapflow/_sgfilt_confound3\".\n",
      "190117-12:22:19,406 nipype.workflow INFO:\n",
      "\t [Node] Running \"_sgfilt_confound3\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190117-12:22:19,436 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_sgfilt_confound3\".\n",
      "190117-12:22:19,441 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_sgfilt_confound4\" in \"/tmp/tmpc7fvcwcc/sgfilt_confound/mapflow/_sgfilt_confound4\".\n",
      "190117-12:22:19,447 nipype.workflow INFO:\n",
      "\t [Node] Running \"_sgfilt_confound4\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190117-12:22:19,485 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_sgfilt_confound4\".\n",
      "190117-12:22:19,492 nipype.workflow INFO:\n",
      "\t [Node] Finished \"sgfilt_confound\".\n"
     ]
    }
   ],
   "source": [
    "res_SG_filter_confounds = SG_filter_confounds.run()\n",
    "#res_SG_filter_confounds.outputs.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert data to percent signal change\n",
    "#define Nodes\n",
    "psc = pe.MapNode(interface=Percent_signal_change, \n",
    "              name='percent_signal_change',\n",
    "             iterfield = ['in_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define settings\n",
    "psc.inputs.func = 'median'\n",
    "psc.inputs.in_file = res_SG_filter.outputs.out_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190117-12:22:27,707 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"percent_signal_change\" in \"/tmp/tmpyc39sd8b/percent_signal_change\".\n",
      "190117-12:22:27,720 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_percent_signal_change0\" in \"/tmp/tmpyc39sd8b/percent_signal_change/mapflow/_percent_signal_change0\".\n",
      "190117-12:22:27,727 nipype.workflow INFO:\n",
      "\t [Node] Running \"_percent_signal_change0\" (\"nipype.interfaces.utility.wrappers.Function\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:37: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190117-12:22:38,869 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_percent_signal_change0\".\n",
      "190117-12:22:38,872 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_percent_signal_change1\" in \"/tmp/tmpyc39sd8b/percent_signal_change/mapflow/_percent_signal_change1\".\n",
      "190117-12:22:38,877 nipype.workflow INFO:\n",
      "\t [Node] Running \"_percent_signal_change1\" (\"nipype.interfaces.utility.wrappers.Function\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:37: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190117-12:22:49,768 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_percent_signal_change1\".\n",
      "190117-12:22:49,772 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_percent_signal_change2\" in \"/tmp/tmpyc39sd8b/percent_signal_change/mapflow/_percent_signal_change2\".\n",
      "190117-12:22:49,777 nipype.workflow INFO:\n",
      "\t [Node] Running \"_percent_signal_change2\" (\"nipype.interfaces.utility.wrappers.Function\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:37: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190117-12:23:00,893 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_percent_signal_change2\".\n",
      "190117-12:23:00,896 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_percent_signal_change3\" in \"/tmp/tmpyc39sd8b/percent_signal_change/mapflow/_percent_signal_change3\".\n",
      "190117-12:23:00,900 nipype.workflow INFO:\n",
      "\t [Node] Running \"_percent_signal_change3\" (\"nipype.interfaces.utility.wrappers.Function\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:37: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190117-12:23:12,463 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_percent_signal_change3\".\n",
      "190117-12:23:12,468 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_percent_signal_change4\" in \"/tmp/tmpyc39sd8b/percent_signal_change/mapflow/_percent_signal_change4\".\n",
      "190117-12:23:12,471 nipype.workflow INFO:\n",
      "\t [Node] Running \"_percent_signal_change4\" (\"nipype.interfaces.utility.wrappers.Function\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:37: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190117-12:23:24,24 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_percent_signal_change4\".\n",
      "190117-12:23:24,29 nipype.workflow INFO:\n",
      "\t [Node] Finished \"percent_signal_change\".\n"
     ]
    }
   ],
   "source": [
    "# run and print outputs\n",
    "res_psc = psc.run()\n",
    "#res_psc.outputs.out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nistats_confound_pca_glm(nifti_file, confounds_file,output_pth):# which_confounds,output_pth):    # function adapted from utils.py in pearl_7T Git\n",
    "    \n",
    "    import pandas as pd\n",
    "    import nibabel as nb\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from nistats.regression import OLSModel\n",
    "    from scipy.stats import zscore\n",
    "    from nilearn.image import math_img\n",
    "    from nilearn.plotting import plot_stat_map\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    infile = nb.load(nifti_file)\n",
    "    mean_img = math_img('np.mean(infile, axis=-1)', infile=infile)\n",
    "    in_data = infile.get_data().astype(np.float32)\n",
    "    \n",
    "    confounds_table = pd.read_csv(confounds_file, sep='\\t', na_values='n/a')#[which_confounds] #select subgroup of confounds\n",
    "    \n",
    "    # Z-SCORING 1ST, THEN OBTAINING COMPONENTS\n",
    "    # we assume the confounds nor the nifti_file need temporal filtering \n",
    "    z_conf = confounds_table.apply(zscore) #do zscore of each value in the sample (scale data matrix)\n",
    "    \n",
    "    pca = PCA(0.95,whiten=True) #choose the minimum number of principal components such that at least 95% of the variance is retained.\n",
    "    pca_confs = pca.fit_transform(np.nan_to_num(z_conf))\n",
    "    num_comp = pca.n_components_\n",
    "\n",
    "    all_confs = pd.DataFrame(pca_confs, columns=['comp_{n}'.format(n=n) for n in range(num_comp)])\n",
    "    all_confs['intercept'] = np.ones(infile.shape[-1]) #add an intercept column filled with ones\n",
    "    \n",
    "    om = OLSModel(np.array(all_confs)) #create a least squares model based on nuisances\n",
    "    om_rr = om.fit(in_data.reshape((-1,infile.shape[-1])).T) #fit bold data to model\n",
    "    \n",
    "    resid_img = nb.Nifti1Image(om_rr.resid.T.reshape(infile.shape).astype(np.float32), affine=infile.affine, header=infile.header)\n",
    "    cleaned_img = math_img('(resid_img + mean_img[...,np.newaxis]).astype(np.float32)', resid_img=resid_img, mean_img=mean_img)\n",
    "    \n",
    "    output_nifti = output_pth+os.path.basename(nifti_file).replace('.nii.gz', '_nuis.nii.gz')\n",
    "    cleaned_img.to_filename(output_nifti)\n",
    "    \n",
    "    output_pdf = output_pth+os.path.basename(confounds_file).replace('.tsv', '_sd-diff.pdf')\n",
    "    f = plt.figure(figsize=(24,6))\n",
    "    plot_stat_map(math_img('(infile.std(axis=-1)-cleaned_img.std(axis=-1))/infile.std(axis=-1)', infile=infile, cleaned_img=cleaned_img), \n",
    "                bg_img=mean_img, figure=f, cut_coords=(0,0,0), threshold=0.125, vmax=1, cmap='viridis', output_file=output_pdf)\n",
    "    \n",
    "    return output_pdf, output_nifti\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # STANDARD SCALING 1ST, THEN OBTAINING COMPONENTS\n",
    "# # we assume the confounds nor the nifti_file need temporal filtering \n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# stand_conf = StandardScaler().fit_transform(np.nan_to_num(confounds_table))\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(0.95,whiten=True) #choose the minimum number of principal components such that 95% of the variance is retained.\n",
    "# pca_confs = pca.fit_transform(np.nan_to_num(stand_conf))\n",
    "# num_comp = pca.n_components_\n",
    "\n",
    "# print(pca.explained_variance_)\n",
    "# print(pca.explained_variance_ratio_)\n",
    "# print(pca.explained_variance_ratio_.cumsum())\n",
    "\n",
    "# all_confs = pd.DataFrame(pca_confs, columns=['comp_{n}'.format(n=n) for n in range(num_comp)])\n",
    "# print(all_confs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # NO SCALING, JUST OBTAINING COMPONENTS\n",
    "# # we assume the confounds nor the nifti_file need temporal filtering \n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(0.95,whiten=True) #choose the minimum number of principal components such that 95% of the variance is retained.\n",
    "# pca_confs = pca.fit_transform(np.nan_to_num(confounds_table))\n",
    "# num_comp = pca.n_components_\n",
    "\n",
    "# print(pca.explained_variance_)\n",
    "# print(pca.explained_variance_ratio_)\n",
    "# print(pca.explained_variance_ratio_.cumsum())\n",
    "\n",
    "# all_confs = pd.DataFrame(pca_confs, columns=['comp_{n}'.format(n=n) for n in range(num_comp)])\n",
    "# print(all_confs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use confounds as nuisance regressors in a GLM\n",
    "#define Node\n",
    "confGLM = pe.MapNode(Function(input_names=['nifti_file', 'confounds_file', 'output_pth'],#'which_confounds','output_pth'], \n",
    "                              output_names=['output_pdf', 'output_nifti'],\n",
    "                             function=nistats_confound_pca_glm),\n",
    "                             name='nistats_confound_pca_glm', \n",
    "                             iterfield=[\"nifti_file\", \"confounds_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define settings\n",
    "#confGLM.inputs.which_confounds = analysis_params['nuisance_columns']\n",
    "confGLM.inputs.nifti_file = res_psc.outputs.out_file\n",
    "confGLM.inputs.confounds_file = res_SG_filter_confounds.outputs.out\n",
    "confGLM.inputs.output_pth = dest_pth+'sub-'+sub_num+'/ses-'+ses_num+'/func/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190117-15:08:19,267 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"nistats_confound_pca_glm\" in \"/tmp/tmpto_hg45v/nistats_confound_pca_glm\".\n",
      "190117-15:08:19,273 nipype.workflow INFO:\n",
      "\t [Node] \"nistats_confound_pca_glm\" found cached.\n"
     ]
    }
   ],
   "source": [
    "# run and print outputs\n",
    "res_confGLM = confGLM.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/neuro/projects/data/derivatives/post_fmriprep/sub-02/ses-01/func/sub-02_ses-01_task-prf_run-01_bold_space-T1w_preproc_sg_psc_nuis.nii.gz',\n",
       " '/home/neuro/projects/data/derivatives/post_fmriprep/sub-02/ses-01/func/sub-02_ses-01_task-prf_run-02_bold_space-T1w_preproc_sg_psc_nuis.nii.gz',\n",
       " '/home/neuro/projects/data/derivatives/post_fmriprep/sub-02/ses-01/func/sub-02_ses-01_task-prf_run-03_bold_space-T1w_preproc_sg_psc_nuis.nii.gz',\n",
       " '/home/neuro/projects/data/derivatives/post_fmriprep/sub-02/ses-01/func/sub-02_ses-01_task-prf_run-04_bold_space-T1w_preproc_sg_psc_nuis.nii.gz',\n",
       " '/home/neuro/projects/data/derivatives/post_fmriprep/sub-02/ses-01/func/sub-02_ses-01_task-prf_run-05_bold_space-T1w_preproc_sg_psc_nuis.nii.gz']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_confGLM.outputs.output_nifti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
