{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tryout script to do the whole pipeline on the prf data of one sub\n",
    "# after try to generalize for all tasks \n",
    "# put in Nipype format (do mapNodes because then I can iterate throught files, nicer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from nilearn import image, plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from spynoza.filtering.nodes import Savgol_filter#, Savgol_filter_confounds\n",
    "from spynoza.conversion.nodes import Percent_signal_change\n",
    "from nipype import Node, Function\n",
    "import nipype.pipeline as pe\n",
    "import nipype.interfaces.io as nio\n",
    "from bids.grabbids import BIDSLayout\n",
    "\n",
    "#from nipype.interfaces import fsl\n",
    "#from nipype.interfaces import freesurfer\n",
    "#from nipype.interfaces.utility import Function, IdentityInterface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save relevant params used in json file, \n",
    "# afterwards just need to call the field of the list/dict\n",
    "import json\n",
    "analysis_params = {}\n",
    "json_info = open('SBref_analysis_params.json','r').read()\n",
    "analysis_params.update(json.loads(json_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions\n",
    "def SGfilt_confound(confounds, tr, polyorder=3, deriv=0, window_length=120):\n",
    "    import pandas as pd\n",
    "    from scipy.signal import savgol_filter\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    confounds_table = pd.read_csv(confounds, sep='\\t', na_values='n/a') #added this line to original spynoza func because it was giving error when str transformed to float\n",
    "    \n",
    "    window = np.int(window_length / tr)\n",
    "\n",
    "    # Window must be odd\n",
    "    if window % 2 == 0:\n",
    "        window += 1\n",
    "\n",
    "    confounds_filt = savgol_filter(confounds_table, window_length=window, polyorder=polyorder,\n",
    "                              deriv=deriv, axis=0, mode='nearest')\n",
    "\n",
    "    new_name = os.path.basename(confounds).split('.')[:-1][0] + '_sg.tsv'\n",
    "    out_file = os.path.abspath(new_name)\n",
    "\n",
    "    confounds_table = np.asarray(confounds_table).astype(np.float64) # needed to convert to float to support next operation\n",
    "    pd.DataFrame(confounds_table - confounds_filt).to_csv(out_file, sep='\\t', index=False)\n",
    "\n",
    "    return out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths and variables\n",
    "sub_list = ['5']\n",
    "ses_list = ['1']\n",
    "task='soma'\n",
    "\n",
    "source_pth = '/home/neuro/projects/data/derivatives/fmriprep/'\n",
    "dest_pth = '/home/neuro/projects/data/derivatives/post_fmriprep/'\n",
    "sub_num = str(sub_list[0]).zfill(2)\n",
    "ses_num = str(ses_list[0]).zfill(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets preprocessed niftis from derivative folder\n",
    "layout = BIDSLayout(source_pth)\n",
    "funcdata = layout.get(subject=sub_num,task=task,extensions='space-T1w_preproc.nii.gz',return_type='file') # list of paths to functionals for this sub\n",
    "confdata = layout.get(subject=sub_num,task=task,extensions='.tsv',return_type='file')  # list of paths to confounds for this sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/neuro/projects/data/derivatives/fmriprep/sub-05/ses-01/func/sub-05_ses-01_task-soma_run-01_bold_space-T1w_preproc.nii.gz',\n",
       " '/home/neuro/projects/data/derivatives/fmriprep/sub-05/ses-01/func/sub-05_ses-01_task-soma_run-03_bold_space-T1w_preproc.nii.gz',\n",
       " '/home/neuro/projects/data/derivatives/fmriprep/sub-05/ses-01/func/sub-05_ses-01_task-soma_run-04_bold_space-T1w_preproc.nii.gz']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some HP filtering (Savitzkyâ€“Golay) on bold + confounds\n",
    "# define Nodes\n",
    "SG_filter = pe.MapNode(interface=Savgol_filter,\n",
    "                       name='savgol_filt',\n",
    "                       iterfield=['in_file'])\n",
    "SG_filter_confounds = pe.MapNode(Function(function=SGfilt_confound),\n",
    "                       name='sgfilt_confound',\n",
    "                       iterfield=['confounds']) #iterfied needs different name because of temp dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define settings, right ones? \n",
    "SG_filter.inputs.polyorder = SG_filter_confounds.inputs.polyorder = analysis_params['sgfilter_polyorder']\n",
    "SG_filter.inputs.deriv = SG_filter_confounds.inputs.deriv = analysis_params['sgfilter_deriv']\n",
    "SG_filter.inputs.window_length = SG_filter_confounds.inputs.window_length = analysis_params['sgfilter_window_length']\n",
    "\n",
    "if (sub_num=='01' or sub_num=='03') and ses_num=='01': # exception for some initial subjects' sessions\n",
    "    SG_filter.inputs.tr = SG_filter_confounds.inputs.tr = 1.5\n",
    "else:\n",
    "    SG_filter.inputs.tr = SG_filter_confounds.inputs.tr = analysis_params['TR'] \n",
    "\n",
    "SG_filter.inputs.in_file = funcdata\n",
    "SG_filter_confounds.inputs.confounds = confdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190123-15:03:58,123 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"savgol_filt\" in \"/tmp/tmp8edefq9j/savgol_filt\".\n",
      "190123-15:03:58,130 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_savgol_filt0\" in \"/tmp/tmp8edefq9j/savgol_filt/mapflow/_savgol_filt0\".\n",
      "190123-15:03:58,133 nipype.workflow INFO:\n",
      "\t [Node] Running \"_savgol_filt0\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190123-15:04:25,44 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_savgol_filt0\".\n",
      "190123-15:04:25,49 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_savgol_filt1\" in \"/tmp/tmp8edefq9j/savgol_filt/mapflow/_savgol_filt1\".\n",
      "190123-15:04:25,53 nipype.workflow INFO:\n",
      "\t [Node] Running \"_savgol_filt1\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190123-15:04:51,448 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_savgol_filt1\".\n",
      "190123-15:04:51,452 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_savgol_filt2\" in \"/tmp/tmp8edefq9j/savgol_filt/mapflow/_savgol_filt2\".\n",
      "190123-15:04:51,457 nipype.workflow INFO:\n",
      "\t [Node] Running \"_savgol_filt2\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190123-15:05:18,25 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_savgol_filt2\".\n",
      "190123-15:05:18,30 nipype.workflow INFO:\n",
      "\t [Node] Finished \"savgol_filt\".\n"
     ]
    }
   ],
   "source": [
    "# run and print outputs\n",
    "res_SG_filter = SG_filter.run()\n",
    "#res_SG_filter.outputs.out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190123-15:05:26,414 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"sgfilt_confound\" in \"/tmp/tmp__m8kd2s/sgfilt_confound\".\n",
      "190123-15:05:26,419 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_sgfilt_confound0\" in \"/tmp/tmp__m8kd2s/sgfilt_confound/mapflow/_sgfilt_confound0\".\n",
      "190123-15:05:26,423 nipype.workflow INFO:\n",
      "\t [Node] Running \"_sgfilt_confound0\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190123-15:05:26,452 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_sgfilt_confound0\".\n",
      "190123-15:05:26,454 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_sgfilt_confound1\" in \"/tmp/tmp__m8kd2s/sgfilt_confound/mapflow/_sgfilt_confound1\".\n",
      "190123-15:05:26,457 nipype.workflow INFO:\n",
      "\t [Node] Running \"_sgfilt_confound1\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190123-15:05:26,487 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_sgfilt_confound1\".\n",
      "190123-15:05:26,489 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_sgfilt_confound2\" in \"/tmp/tmp__m8kd2s/sgfilt_confound/mapflow/_sgfilt_confound2\".\n",
      "190123-15:05:26,492 nipype.workflow INFO:\n",
      "\t [Node] Running \"_sgfilt_confound2\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190123-15:05:26,511 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_sgfilt_confound2\".\n",
      "190123-15:05:26,514 nipype.workflow INFO:\n",
      "\t [Node] Finished \"sgfilt_confound\".\n"
     ]
    }
   ],
   "source": [
    "res_SG_filter_confounds = SG_filter_confounds.run()\n",
    "#res_SG_filter_confounds.outputs.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert data to percent signal change\n",
    "#define Nodes\n",
    "psc = pe.MapNode(interface=Percent_signal_change, \n",
    "              name='percent_signal_change',\n",
    "             iterfield = ['in_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define settings\n",
    "psc.inputs.func = 'median'\n",
    "psc.inputs.in_file = res_SG_filter.outputs.out_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190123-15:05:33,806 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"percent_signal_change\" in \"/tmp/tmpqerd1kmk/percent_signal_change\".\n",
      "190123-15:05:33,813 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_percent_signal_change0\" in \"/tmp/tmpqerd1kmk/percent_signal_change/mapflow/_percent_signal_change0\".\n",
      "190123-15:05:33,817 nipype.workflow INFO:\n",
      "\t [Node] Running \"_percent_signal_change0\" (\"nipype.interfaces.utility.wrappers.Function\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:37: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190123-15:05:57,142 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_percent_signal_change0\".\n",
      "190123-15:05:57,144 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_percent_signal_change1\" in \"/tmp/tmpqerd1kmk/percent_signal_change/mapflow/_percent_signal_change1\".\n",
      "190123-15:05:57,147 nipype.workflow INFO:\n",
      "\t [Node] Running \"_percent_signal_change1\" (\"nipype.interfaces.utility.wrappers.Function\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:37: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190123-15:06:19,54 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_percent_signal_change1\".\n",
      "190123-15:06:19,56 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_percent_signal_change2\" in \"/tmp/tmpqerd1kmk/percent_signal_change/mapflow/_percent_signal_change2\".\n",
      "190123-15:06:19,59 nipype.workflow INFO:\n",
      "\t [Node] Running \"_percent_signal_change2\" (\"nipype.interfaces.utility.wrappers.Function\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:37: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190123-15:06:40,936 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_percent_signal_change2\".\n",
      "190123-15:06:40,941 nipype.workflow INFO:\n",
      "\t [Node] Finished \"percent_signal_change\".\n"
     ]
    }
   ],
   "source": [
    "# run and print outputs\n",
    "res_psc = psc.run()\n",
    "#res_psc.outputs.out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nistats_confound_pca_glm(nifti_file, confounds_file,output_pth):# which_confounds,output_pth):    # function adapted from utils.py in pearl_7T Git\n",
    "    \n",
    "    import pandas as pd\n",
    "    import nibabel as nb\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from nistats.regression import OLSModel\n",
    "    from scipy.stats import zscore\n",
    "    from nilearn.image import math_img\n",
    "    from nilearn.plotting import plot_stat_map\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    infile = nb.load(nifti_file)\n",
    "    mean_img = math_img('np.mean(infile, axis=-1)', infile=infile)\n",
    "    in_data = infile.get_data().astype(np.float32)\n",
    "    \n",
    "    confounds_table = pd.read_csv(confounds_file, sep='\\t', na_values='n/a')#[which_confounds] #select subgroup of confounds\n",
    "    \n",
    "    # Z-SCORING 1ST, THEN OBTAINING COMPONENTS\n",
    "    # we assume the confounds nor the nifti_file need temporal filtering \n",
    "    z_conf = confounds_table.apply(zscore) #do zscore of each value in the sample (scale data matrix)\n",
    "    \n",
    "    pca = PCA(0.95,whiten=True) #choose the minimum number of principal components such that at least 95% of the variance is retained.\n",
    "    pca_confs = pca.fit_transform(np.nan_to_num(z_conf))\n",
    "    num_comp = pca.n_components_\n",
    "\n",
    "    all_confs = pd.DataFrame(pca_confs, columns=['comp_{n}'.format(n=n) for n in range(num_comp)])\n",
    "    all_confs['intercept'] = np.ones(infile.shape[-1]) #add an intercept column filled with ones\n",
    "    \n",
    "    om = OLSModel(np.array(all_confs)) #create a least squares model based on nuisances\n",
    "    om_rr = om.fit(in_data.reshape((-1,infile.shape[-1])).T) #fit bold data to model\n",
    "    \n",
    "    resid_img = nb.Nifti1Image(om_rr.resid.T.reshape(infile.shape).astype(np.float32), affine=infile.affine, header=infile.header)\n",
    "    cleaned_img = math_img('(resid_img + mean_img[...,np.newaxis]).astype(np.float32)', resid_img=resid_img, mean_img=mean_img)\n",
    "    \n",
    "    output_nifti = output_pth+os.path.basename(nifti_file).replace('.nii.gz', '_nuis.nii.gz')\n",
    "    cleaned_img.to_filename(output_nifti)\n",
    "    \n",
    "    output_pdf = output_pth+os.path.basename(confounds_file).replace('.tsv', '_sd-diff.pdf')\n",
    "    f = plt.figure(figsize=(24,6))\n",
    "    plot_stat_map(math_img('(infile.std(axis=-1)-cleaned_img.std(axis=-1))/infile.std(axis=-1)', infile=infile, cleaned_img=cleaned_img), \n",
    "                bg_img=mean_img, figure=f, cut_coords=(0,0,0), threshold=0.125, vmax=1, cmap='viridis', output_file=output_pdf)\n",
    "    \n",
    "    return output_pdf, output_nifti\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # STANDARD SCALING 1ST, THEN OBTAINING COMPONENTS\n",
    "# # we assume the confounds nor the nifti_file need temporal filtering \n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# stand_conf = StandardScaler().fit_transform(np.nan_to_num(confounds_table))\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(0.95,whiten=True) #choose the minimum number of principal components such that 95% of the variance is retained.\n",
    "# pca_confs = pca.fit_transform(np.nan_to_num(stand_conf))\n",
    "# num_comp = pca.n_components_\n",
    "\n",
    "# print(pca.explained_variance_)\n",
    "# print(pca.explained_variance_ratio_)\n",
    "# print(pca.explained_variance_ratio_.cumsum())\n",
    "\n",
    "# all_confs = pd.DataFrame(pca_confs, columns=['comp_{n}'.format(n=n) for n in range(num_comp)])\n",
    "# print(all_confs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # NO SCALING, JUST OBTAINING COMPONENTS\n",
    "# # we assume the confounds nor the nifti_file need temporal filtering \n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(0.95,whiten=True) #choose the minimum number of principal components such that 95% of the variance is retained.\n",
    "# pca_confs = pca.fit_transform(np.nan_to_num(confounds_table))\n",
    "# num_comp = pca.n_components_\n",
    "\n",
    "# print(pca.explained_variance_)\n",
    "# print(pca.explained_variance_ratio_)\n",
    "# print(pca.explained_variance_ratio_.cumsum())\n",
    "\n",
    "# all_confs = pd.DataFrame(pca_confs, columns=['comp_{n}'.format(n=n) for n in range(num_comp)])\n",
    "# print(all_confs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use confounds as nuisance regressors in a GLM\n",
    "#define Node\n",
    "confGLM = pe.MapNode(Function(input_names=['nifti_file', 'confounds_file', 'output_pth'],#'which_confounds','output_pth'], \n",
    "                              output_names=['output_pdf', 'output_nifti'],\n",
    "                             function=nistats_confound_pca_glm),\n",
    "                             name='nistats_confound_pca_glm', \n",
    "                             iterfield=[\"nifti_file\", \"confounds_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define settings\n",
    "#confGLM.inputs.which_confounds = analysis_params['nuisance_columns']\n",
    "confGLM.inputs.nifti_file = res_psc.outputs.out_file\n",
    "confGLM.inputs.confounds_file = res_SG_filter_confounds.outputs.out\n",
    "confGLM.inputs.output_pth = dest_pth+'sub-'+sub_num+'/ses-'+ses_num+'/func/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190123-15:07:15,585 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"nistats_confound_pca_glm\" in \"/tmp/tmptdq26ju5/nistats_confound_pca_glm\".\n",
      "190123-15:07:15,592 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_nistats_confound_pca_glm0\" in \"/tmp/tmptdq26ju5/nistats_confound_pca_glm/mapflow/_nistats_confound_pca_glm0\".\n",
      "190123-15:07:15,596 nipype.workflow INFO:\n",
      "\t [Node] Running \"_nistats_confound_pca_glm0\" (\"nipype.interfaces.utility.wrappers.Function\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda-latest/envs/neuro/lib/python3.6/site-packages/numpy/core/_methods.py:138: RuntimeWarning: invalid value encountered in sqrt\n",
      "  ret = um.sqrt(ret, out=ret)\n",
      "<string>:1: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190123-15:07:42,929 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_nistats_confound_pca_glm0\".\n",
      "190123-15:07:42,932 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_nistats_confound_pca_glm1\" in \"/tmp/tmptdq26ju5/nistats_confound_pca_glm/mapflow/_nistats_confound_pca_glm1\".\n",
      "190123-15:07:42,935 nipype.workflow INFO:\n",
      "\t [Node] Running \"_nistats_confound_pca_glm1\" (\"nipype.interfaces.utility.wrappers.Function\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:1: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190123-15:08:10,434 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_nistats_confound_pca_glm1\".\n",
      "190123-15:08:10,437 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"_nistats_confound_pca_glm2\" in \"/tmp/tmptdq26ju5/nistats_confound_pca_glm/mapflow/_nistats_confound_pca_glm2\".\n",
      "190123-15:08:10,440 nipype.workflow INFO:\n",
      "\t [Node] Running \"_nistats_confound_pca_glm2\" (\"nipype.interfaces.utility.wrappers.Function\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:1: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190123-15:08:37,340 nipype.workflow INFO:\n",
      "\t [Node] Finished \"_nistats_confound_pca_glm2\".\n",
      "190123-15:08:37,344 nipype.workflow INFO:\n",
      "\t [Node] Finished \"nistats_confound_pca_glm\".\n"
     ]
    }
   ],
   "source": [
    "# run and print outputs\n",
    "res_confGLM = confGLM.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/neuro/projects/data/derivatives/post_fmriprep/sub-05/ses-01/func/sub-05_ses-01_task-soma_run-01_bold_space-T1w_preproc_sg_psc_nuis.nii.gz',\n",
       " '/home/neuro/projects/data/derivatives/post_fmriprep/sub-05/ses-01/func/sub-05_ses-01_task-soma_run-03_bold_space-T1w_preproc_sg_psc_nuis.nii.gz',\n",
       " '/home/neuro/projects/data/derivatives/post_fmriprep/sub-05/ses-01/func/sub-05_ses-01_task-soma_run-04_bold_space-T1w_preproc_sg_psc_nuis.nii.gz']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_confGLM.outputs.output_nifti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_one_out_lists(input_list):\n",
    "    \"\"\"leave_one_out_lists takes creates a list of lists, with each element\n",
    "    of the input_list left out of the returned lists once, in order.\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_list : list\n",
    "        list of items, for instance absolute paths to nii files\n",
    "    Returns\n",
    "    -------\n",
    "    output_data : list\n",
    "        list of lists\n",
    "    \"\"\"\n",
    "\n",
    "    out_lists = []\n",
    "    for x in input_list:\n",
    "        out_lists.append([y for y in input_list if y != x])\n",
    "\n",
    "    return out_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['/home/neuro/projects/data/derivatives/post_fmriprep/sub-02/ses-01/func/sub-02_ses-01_task-soma_run-02_bold_space-T1w_preproc_sg_psc_nuis.nii.gz',\n",
       "  '/home/neuro/projects/data/derivatives/post_fmriprep/sub-02/ses-01/func/sub-02_ses-01_task-soma_run-03_bold_space-T1w_preproc_sg_psc_nuis.nii.gz',\n",
       "  '/home/neuro/projects/data/derivatives/post_fmriprep/sub-02/ses-01/func/sub-02_ses-01_task-soma_run-04_bold_space-T1w_preproc_sg_psc_nuis.nii.gz'],\n",
       " ['/home/neuro/projects/data/derivatives/post_fmriprep/sub-02/ses-01/func/sub-02_ses-01_task-soma_run-01_bold_space-T1w_preproc_sg_psc_nuis.nii.gz',\n",
       "  '/home/neuro/projects/data/derivatives/post_fmriprep/sub-02/ses-01/func/sub-02_ses-01_task-soma_run-03_bold_space-T1w_preproc_sg_psc_nuis.nii.gz',\n",
       "  '/home/neuro/projects/data/derivatives/post_fmriprep/sub-02/ses-01/func/sub-02_ses-01_task-soma_run-04_bold_space-T1w_preproc_sg_psc_nuis.nii.gz'],\n",
       " ['/home/neuro/projects/data/derivatives/post_fmriprep/sub-02/ses-01/func/sub-02_ses-01_task-soma_run-01_bold_space-T1w_preproc_sg_psc_nuis.nii.gz',\n",
       "  '/home/neuro/projects/data/derivatives/post_fmriprep/sub-02/ses-01/func/sub-02_ses-01_task-soma_run-02_bold_space-T1w_preproc_sg_psc_nuis.nii.gz',\n",
       "  '/home/neuro/projects/data/derivatives/post_fmriprep/sub-02/ses-01/func/sub-02_ses-01_task-soma_run-04_bold_space-T1w_preproc_sg_psc_nuis.nii.gz'],\n",
       " ['/home/neuro/projects/data/derivatives/post_fmriprep/sub-02/ses-01/func/sub-02_ses-01_task-soma_run-01_bold_space-T1w_preproc_sg_psc_nuis.nii.gz',\n",
       "  '/home/neuro/projects/data/derivatives/post_fmriprep/sub-02/ses-01/func/sub-02_ses-01_task-soma_run-02_bold_space-T1w_preproc_sg_psc_nuis.nii.gz',\n",
       "  '/home/neuro/projects/data/derivatives/post_fmriprep/sub-02/ses-01/func/sub-02_ses-01_task-soma_run-03_bold_space-T1w_preproc_sg_psc_nuis.nii.gz']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leave_one_out_lists(res_confGLM.outputs.output_nifti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY TO LOOK AT SOMA DATA ############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# somas = res_confGLM.outputs.output_nifti # list of residual niftis\n",
    "# mean_filename = (res_confGLM.outputs.output_nifti[0]).replace('run-01', 'run-mean')\n",
    "\n",
    "# mean_soma=image.math_img('(i1+i2+i3+i4)/4', i1=somas[0], i2=somas[1], i3=somas[2],i4=somas[3])\n",
    "# mean_soma.to_filename(mean_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nr_stims = len(analysis_params['soma_stimulus']) #number of stim videos\n",
    "# stimulus_setup = np.array([s.split('.avi')[0] for s in analysis_params['soma_stimulus']]) # name for all regressors\n",
    "# different_regressors = np.unique(stimulus_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  12.  ,   14.75,   17.5 ,   20.25,   23.  ,   25.75,   28.5 ,\n",
       "         31.25,   34.  ,   36.75,   39.5 ,   42.25,   45.  ,   47.75,\n",
       "         50.5 ,   53.25,   56.  ,   58.75,   61.5 ,   64.25,   67.  ,\n",
       "         69.75,   72.5 ,   75.25,   78.  ,   80.75,   83.5 ,   86.25,\n",
       "         89.  ,   91.75,   94.5 ,   97.25,  100.  ,  102.75,  105.5 ,\n",
       "        108.25,  111.  ,  113.75,  116.5 ,  119.25,  122.  ,  124.75,\n",
       "        127.5 ,  130.25,  133.  ,  135.75,  138.5 ,  141.25,  144.  ,\n",
       "        146.75,  149.5 ,  152.25,  155.  ,  157.75,  160.5 ,  163.25,\n",
       "        166.  ,  168.75,  171.5 ,  174.25])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# onset_times = np.arange(0, 2.75*nr_stims, 2.75) + 12\n",
    "# onset_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bhand_fing1': array([  50.5,  144. ]),\n",
       " 'bhand_fing2': array([  53.25,  141.25]),\n",
       " 'bhand_fing3': array([  56. ,  138.5]),\n",
       " 'bhand_fing4': array([  58.75,  135.75]),\n",
       " 'bhand_fing5': array([  61.5,  133. ]),\n",
       " 'bleg': array([  64.25,  146.75]),\n",
       " 'eyebrows': array([  12.  ,   39.5 ,   75.25,   94.5 ,  122.  ,  157.75]),\n",
       " 'eyes': array([  14.75,   42.25,   72.5 ,   97.25,  124.75,  155.  ]),\n",
       " 'lhand_fing1': array([  23. ,  116.5]),\n",
       " 'lhand_fing2': array([  25.75,  113.75]),\n",
       " 'lhand_fing3': array([  28.5,  111. ]),\n",
       " 'lhand_fing4': array([  31.25,  108.25]),\n",
       " 'lhand_fing5': array([  34. ,  105.5]),\n",
       " 'lleg': array([  36.75,  119.25]),\n",
       " 'mouth': array([  17.5 ,   45.  ,   69.75,  100.  ,  127.5 ,  152.25]),\n",
       " 'rhand_fing1': array([  78. ,  171.5]),\n",
       " 'rhand_fing2': array([  80.75,  168.75]),\n",
       " 'rhand_fing3': array([  83.5,  166. ]),\n",
       " 'rhand_fing4': array([  86.25,  163.25]),\n",
       " 'rhand_fing5': array([  89. ,  160.5]),\n",
       " 'rleg': array([  91.75,  174.25]),\n",
       " 'tongue': array([  20.25,   47.75,   67.  ,  102.75,  130.25,  149.5 ])}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {er: onset_times[stimulus_setup==er] for er in different_regressors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# stim_onset_list = []\n",
    "# for i, s in enumerate(stimulus_setup):\n",
    "#     if s[0] == 'b':\n",
    "#         stim_onset_list.append([onset_times[i], 2.25, 'l'+s[1:]])\n",
    "#         stim_onset_list.append([onset_times[i], 2.25, 'r'+s[1:]])        \n",
    "#     stim_onset_list.append([onset_times[i], 2.25, s])\n",
    "# events = pd.DataFrame(stim_onset_list, columns=['onset','duration','trial_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225.60000000000002"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "141*1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
